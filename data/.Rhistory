pca.fit <- prcomp(features.only, scale. = TRUE)
pca.results <- summary(pca.fit)
pca.cdf <- pca.results$importance[3, 1:6]
pca.cdf
plot(pca.cdf)
# Return Data
data.pca <- pca.results$x
dim(data.pca)
df.pca <- data.frame(data.pca)
#########################################################
# Kmeans
#########################################################
fit.kmeans <- kmeans(df.pca, 6, nstart=25)
# Add Back Earned & Incurred
df.pca$Cluster <- fit.kmeans$cluster
df.pca$Incurred <- data.subset$Incurred
df.pca$Earned <- data.subset$Earned
df.grouped <- df.pca %>% group_by(Cluster)%>% summarise(sum_incurred = sum(Incurred),
sum_earned = sum(Earned),
cnt = n())
df.grouped$LossRatio <- df.grouped$sum_incurred / df.grouped$sum_earned
df.grouped
df.grouped$PctNum <- df.grouped$cnt / sum(df.grouped$cnt)
df.grouped
df.grouped$PctNum <- df.grouped$cnt / sum(df.grouped$cnt) * 100
df.grouped
clusplot(df.pca, fit.kmeans$cluster, color=TRUE, shade=TRUE,
labels=0, lines=0)
fviz_cluster(fit.kmeans, data = df.pca)
#########################################################
# Kmeans
#########################################################
fit.kmeans <- kmeans(df.pca, 4, nstart=25)
fviz_cluster(fit.kmeans, data = df.pca)
fviz_cluster(fit.kmeans, data = df.pca)
clusplot(df.pca, fit.kmeans$cluster, color=TRUE, shade=TRUE,
labels=0, lines=0)
#########################################################
# Kmeans
#########################################################
fit.kmeans <- kmeans(df.pca, 4, nstart=25)
df.pca$Cluster <- fit.kmeans$cluster
df.pca$Incurred <- data.subset$Incurred
df.pca$Earned <- data.subset$Earned
df.grouped <- df.pca %>% group_by(Cluster)%>% summarise(sum_incurred = sum(Incurred),
sum_earned = sum(Earned),
cnt = n())
df.grouped$LossRatio <- df.grouped$sum_incurred / df.grouped$sum_earned
df.grouped$PctNum <- df.grouped$cnt / sum(df.grouped$cnt) * 100
df.grouped
#########################################################
# Kmeans
#########################################################
fit.kmeans <- kmeans(df.pca, 10, nstart=25)
clusplot(df.pca, fit.kmeans$cluster, color=TRUE, shade=TRUE,
labels=0, lines=0)
fviz_cluster(fit.kmeans, data = df.pca)
df.pca$Cluster <- fit.kmeans$cluster
df.pca$Incurred <- data.subset$Incurred
df.pca$Earned <- data.subset$Earned
df.grouped <- df.pca %>% group_by(Cluster)%>% summarise(sum_incurred = sum(Incurred),
sum_earned = sum(Earned),
cnt = n())
df.grouped$LossRatio <- df.grouped$sum_incurred / df.grouped$sum_earned
df.grouped$PctNum <- df.grouped$cnt / sum(df.grouped$cnt) * 100
df.grouped
#########################################################
# Kmeans
#########################################################
fit.kmeans <- kmeans(df.pca, 3, nstart=25)
clusplot(df.pca, fit.kmeans$cluster, color=TRUE, shade=TRUE,
labels=0, lines=0)
df.pca <- subset(df.pca, select = -c(PC3, PC4, PC5, PC6))
#########################################################
# Kmeans
#########################################################
fit.kmeans <- kmeans(df.pca, 3, nstart=25)
clusplot(df.pca, fit.kmeans$cluster, color=TRUE, shade=TRUE,
labels=0, lines=0)
#########################################################
# Kmeans
#########################################################
fit.kmeans <- kmeans(df.pca, 4, nstart=25)
clusplot(df.pca, fit.kmeans$cluster, color=TRUE, shade=TRUE,
labels=0, lines=0)
#########################################################
# Kmeans
#########################################################
fit.kmeans <- kmeans(df.pca, 6, nstart=25)
clusplot(df.pca, fit.kmeans$cluster, color=TRUE, shade=TRUE,
labels=0, lines=0)
fviz_cluster(fit.kmeans, data = df.pca)
# Add Back Earned & Incurred
df.pca$Cluster <- fit.kmeans$cluster
df.pca$Incurred <- data.subset$Incurred
df.pca$Earned <- data.subset$Earned
df.grouped <- df.pca %>% group_by(Cluster)%>% summarise(sum_incurred = sum(Incurred),
sum_earned = sum(Earned),
cnt = n())
df.grouped$LossRatio <- df.grouped$sum_incurred / df.grouped$sum_earned
df.grouped$PctNum <- df.grouped$cnt / sum(df.grouped$cnt) * 100
df.grouped
library(data.table)
DT = data.table(
ID = c("b","b","b","a","a","c"),
a = 1:6,
b = 7:12,
c = 13:18)
DT
dt<- data.table(
id = c("b","b","b","a","a","c"),
a = 1:6,
b = 7:12,
c = 13:18)
dt
dt[, surplus := TRUE]
dt
devtools::install_github("jakobbossek/ecr2")
pkgbuild::check_build_tools(debug = TRUE)
devtools::install_github("jakobbossek/ecr")
install.library(devtools)
install.packages("devtools")
install.packages("renv")
renv::install("jakobbossek/ecr2")
library(ecr)
# Find Global Minima of Ackley-Funciton
fn = makeAckleyFunction(1L)
autoplot(fn, show.optimum=TRUE, length.out = 1000)
library(ggplot2)
library(smoof)
# Find Global Minima of Ackley-Funciton
fn = makeAckleyFunction(1L)
autoplot(fn, show.optimum=TRUE, length.out = 1000)
library(readtext)
library(quanteda)
library(text2vec)
library(sentimentr)
library(tidytext)
library(textdata)
dir_base = 'C:\\Users\\chris.cirelli\\Desktop\\repositories\\gsu_legal_analytics_course'
dir_data = file.path(dir_base, '\\data')
dir_hw = file.path(dir_base, '\\hw\\hw4')
setwd(dir_hw)
################################################################################
# Load Data
################################################################################
nlra.txt <- readtext(paste0(dir_data, "\\nlra.txt"))
################################################################################
# 1.) Get Tokens of Interest
################################################################################
nlra.corpus <- corpus(nlra.txt$text)
nlra.toks <- tokens(nlra.coprus)
################################################################################
# 1.) Get Tokens of Interest
################################################################################
nlra.corpus <- corpus(nlra.txt$text)
nlra.corpus
nlra.toks <- tokens(nlra.corpus)
View(nlra.toks)
nlra.dfm <- dfm(nlra.corpus)
View(nlra.dfm)
nlra.tf <- textstat_frequency(nlra.dfm)
View(nlra.tf)
en.sw <- stopwords("en")
en.sw
en.sw <- stopwords("en")
nlra.dfm <- dfm(nlra.corpus,
remove_punct=TRUE,
remove_numbers=TRUE,
remove_symbols=TRUE)
nlra.tf <- textstat_frequency(nlra.dfm)
View(nlra.tf)
nlra.dfm <- dfm(nlra.corpus,
remove=en.sw,
remove_punct=TRUE,
remove_numbers=TRUE,
remove_symbols=TRUE)
nlra.tf <- textstat_frequency(nlra.dfm)
View(nlra.tf)
help(replace_non_ascii)
nlra.corpus
################################################################################
# 1.) Get Tokens of Interest
################################################################################
nlra.ascii <- replace_non_ascii(nlra.txt, replacement="", remove.nonconverted=TRUE)
################################################################################
# 1.) Get Tokens of Interest
################################################################################
nlra.ascii <- replace_non_ascii(nlra.txt, replacement="", remove.nonconverted=TRUE)
library(textclean)
################################################################################
# 1.) Get Tokens of Interest
################################################################################
nlra.ascii <- replace_non_ascii(nlra.txt, replacement="", remove.nonconverted=TRUE)
head(nlra.ascii)
nlra.corpus <- corpus(nlra.ascii$text)
nlra.ascii
nlra.corpus <- corpus(nlra.ascii)
nlra.toks <- tokens(nlra.corpus)
en.sw <- stopwords("en")
nlra.dfm <- dfm(nlra.corpus,
remove=en.sw,
remove_punct=TRUE,
remove_numbers=TRUE,
remove_symbols=TRUE)
nlra.tf <- textstat_frequency(nlra.dfm)
View(nlra.tf)
nlra.dfm <- dfm(nlra.corpus,
remove=en.sw,
remove_punct=TRUE,
remove_numbers=TRUE,
remove_symbols=TRUE,
stem=TRUE)
nlra.tf <- textstat_frequency(nlra.dfm)
View(nlra.tf)
View(nlra.tf$feature)
nlra.tf$feature[1:10]
nlra.tf$feature[1:10:1:2]
nlra.tf$feature[1:10, 1:2]
nlra.tf$feature[1:10, 1]
nlra.tf$feature[1:10]
head(nlra.tf$feature)
head(nlra.tf)
head(nlra.tf, n=10)
head(nlra.tf, n=20)
head(nlra.tf, n=100)
tk.of.int <- c('labor', 'employe', 'employ', 'board', 'servic', 'unfair',
'collect', 'concili', 'right')
tk.of.int
nlra.tf <- textstat_frequency(nlra.1gram.dfm)
nlra.1gram.dfm <- dfm(nlra.corpus,
remove=en.sw,
remove_punct=TRUE,
remove_numbers=TRUE,
remove_symbols=TRUE,
stem=TRUE)
nlra.tf <- textstat_frequency(nlra.1gram.dfm)
head(nlra.tf, n=100)
tk.of.int.1gram <- c('labor', 'employe', 'employ', 'board', 'servic', 'unfair',
'collect', 'concili', 'right')
nlra.toks.1gram <- tokens(nlra.corpus)
nlra.toks.2gram <- tokens_ngrams(nlra.toks.1gram, n=2)
head(nlra.toks.2gram)
nlra.tf.1gram <- textstat_frequency(nlra.1gram.dfm)
head(nlra.tf.1gram, n=100)
tk.of.int.1gram <- c('labor', 'employe', 'employ', 'board', 'servic', 'unfair',
'collect', 'concili', 'right')
nlra.1gram.dfm <- dfm(nlra.toks.1gram,
remove=en.sw,
remove_punct=TRUE,
remove_numbers=TRUE,
remove_symbols=TRUE,
stem=TRUE)
ii(nlra.txt, replacement="", remove.nonconverted=TRUE)
nlra.corpus <- corpus(nlra.ascii)
nlra.tk.1gram <- tokens(nlra.corpus)
nlra.tk.2gram <- tokens_ngrams(nlra.tk.1gram, n=2)
en.sw <- stopwords("en")
# Get 1 Gram Dfm
nlra.1gram.dfm <- dfm(nlra.tk.1gram,
remove=en.sw,
remove_punct=TRUE,
remove_numbers=TRUE,
remove_symbols=TRUE,
stem=TRUE)
nlra.ascii <- replace_non_ascii(nlra.txt, replacement="", remove.nonconverted=TRUE)
nlra.corpus <- corpus(nlra.ascii)
nlra.tk.1gram <- tokens(nlra.corpus)
nlra.tk.2gram <- tokens_ngrams(nlra.tk.1gram, n=2)
en.sw <- stopwords("en")
# Get 1 Gram Dfm
nlra.1gram.dfm <- dfm(nlra.tk.1gram,
remove=en.sw,
remove_punct=TRUE,
remove_numbers=TRUE,
remove_symbols=TRUE,
stem=TRUE)
nlra.tf.1gram <- textstat_frequency(nlra.1gram.dfm)
head(nlra.tf.1gram, n=100)
tk.of.int.1gram <- c('labor', 'employe', 'employ', 'board', 'servic', 'unfair',
'collect', 'concili', 'right')
nlra.2gram.dfm <- dfm(nlra.tk.2gram,
remove=en.sw,
remove_punct=TRUE,
remove_numbers=TRUE,
remove_symbols=TRUE,
stem=TRUE)
nlra.tf.2gram <- textstat_frequency(nlra.2gram.dfm)
head(nlra.tf.2gram, n=100)
nlra.tk.1gram <- tokens(nlra.corpus, remove_punct=TRUE, remove_symbols=TRUE,
remove_numbers=TRUE)
nlra.tk.2gram <- tokens_ngrams(nlra.tk.1gram, n=2)
en.sw <- stopwords("en")
# Get 1 Gram Dfm
nlra.1gram.dfm <- dfm(nlra.tk.1gram,
remove=en.sw,
remove_punct=TRUE,
remove_numbers=TRUE,
remove_symbols=TRUE,
stem=TRUE)
nlra.tf.1gram <- textstat_frequency(nlra.1gram.dfm)
head(nlra.tf.1gram, n=100)
tk.of.int.1gram <- c('labor', 'employe', 'employ', 'board', 'servic', 'unfair',
'collect', 'concili', 'right')
nlra.2gram.dfm <- dfm(nlra.tk.2gram,
remove=en.sw,
remove_punct=TRUE,
remove_numbers=TRUE,
remove_symbols=TRUE,
stem=TRUE)
nlra.tf.2gram <- textstat_frequency(nlra.2gram.dfm)
head(nlra.tf.2gram, n=100)
nlra.tk.1gram <- tokens(nlra.corpus, remove_punct=TRUE, remove_symbols=TRUE,
remove_numbers=TRUE, remove=en.sw)
nlra.tk.1gram <- tokens_select(tokens(nlra.corpus, remove_punct=TRUE, remove_symbols=TRUE,
remove_numbers=TRUE), pattern=en.sw)
# Get 1 Gram Dfm
nlra.1gram.dfm <- dfm(nlra.tk.1gram)
nlra.tf.1gram <- textstat_frequency(nlra.1gram.dfm)
head(nlra.tf.1gram, n=100)
nlra.tk.1gram <- tokens_wordstem(
tokens_select(
tokens(nlra.corpus, remove_punct=TRUE,
remove_symbols=TRUE, remove_numbers=TRUE),pattern=en.sw))
nlra.tk.1gram <- tokens_wordstem(
tokens_select(
tokens(nlra.corpus, remove_punct=TRUE,
remove_symbols=TRUE, remove_numbers=TRUE),
pattern=en.sw)
)
nlra.tk.2gram <- tokens_ngrams(nlra.tk.1gram, n=2)
nlra.1gram.dfm <- dfm(nlra.tk.1gram)
nlra.tf.1gram <- textstat_frequency(nlra.1gram.dfm)
head(nlra.tf.1gram, n=100)
nlra.2gram.dfm <- dfm(nlra.tk.2gram)
nlra.tf.2gram <- textstat_frequency(nlra.2gram.dfm)
head(nlra.tf.2gram, n=100)
nlra.tk.1gram <- tokens_wordstem(
tokens_select(
tokens(nlra.corpus, remove_punct=TRUE,
remove_symbols=TRUE, remove_numbers=TRUE),
pattern=en.sw, selection="remove")
)
nlra.tk.2gram <- tokens_ngrams(nlra.tk.1gram, n=2)
nlra.1gram.dfm <- dfm(nlra.tk.1gram)
nlra.tf.1gram <- textstat_frequency(nlra.1gram.dfm)
head(nlra.tf.1gram, n=100)
# Get Bi-Gram Dfm
nlra.2gram.dfm <- dfm(nlra.tk.2gram)
nlra.tf.2gram <- textstat_frequency(nlra.2gram.dfm)
head(nlra.tf.2gram, n=100)
nlra.tf.2gram$feature[1:100]
nlra.tf.2gram$feature[1:100]
tk.of.int.2gram <- c('labor_organ', 'unfair_labor', 'labor_practic',
'repres_employe', 'labor_disput', 'collect_bargain',
'employe_employ', 'nation_labor')
cr.txt <- readtext('*crec')
setwd(dir_data)
cr.txt <- readtext('*crec')
cr.txt <- readtext("*crec")
cr.txt <- readtext("crec*")
cr.txt
cr_corpus = corpus(cr.txt)
View(cr_corpus)
ggplot(nlra.tf.1gram[1:20, ], aes(x = reorder(feature, frequency), y = frequency)) +
geom_point() + coord_flip() + labs(x = NULL, y = "Frequency")
library(ggplot2)
ggplot(nlra.tf.1gram[1:20, ], aes(x = reorder(feature, frequency), y = frequency)) +
geom_point() + coord_flip() + labs(x = NULL, y = "Frequency")
ggplot(nlra.tf.2gram[1:20, ], aes(x = reorder(feature, frequency), y = frequency)) +
geom_point() + coord_flip() + labs(x = NULL, y = "Frequency")
cr.tks <- tokens(cr_corpus)
cr.tk.1gram <- tokens_wordstem(
tokens_select(
tokens(nlra.corpus, remove_punct=TRUE,
remove_symbols=TRUE, remove_numbers=TRUE),
pattern=en.sw, selection="remove")
)
cr.dfm <- dfm(cr.corpus)
cr.corpus = corpus(cr.txt)
cr.tk.1gram <- tokens_wordstem(
tokens_select(
tokens(nlra.corpus, remove_punct=TRUE,
remove_symbols=TRUE, remove_numbers=TRUE),
pattern=en.sw, selection="remove")
)
cr.dfm <- dfm(cr.corpus)
summary(cr.dfm)
cr.dfm <- dfm(cr.tk.1gram)
summary(cr.dfm)
head(cr.dfm)
View(cr.dfm)
length(cr.txt)
length(cr.txt$text)
################################################################################
# 1.) Get Tokens of Interest
################################################################################
en.sw <- stopwords("en")
cr.corpus = corpus(cr.txt)
View(cr.corpus)
cr.dfm.kw <- dfm_select(cr.tk.1gram, pattern=tk.of.int.1gram)
cr.dfm.kw <- dfm_select(cr.dfm, pattern=tk.of.int.1gram)
cr.dfm.kw
cr.dfm.1gram <- dfm(cr.tk.1gram)
cr.dfm.kw <- dfm_select(cr.dfm.1gram, pattern=tk.of.int.1gram)
cr.tf.1gram <- textstat_frequency(cr.dfm.1gram)
cr.tf.1gram
cr.dfm.kw <- dfm_select(cr.dfm.1gram, pattern=tk.of.int.1gram)
cr.tf.1gram <- textstat_frequency(cr.dfm.1gram)
cr.tf.1gram
cr.dfm.kw <- dfm_select(cr.dfm.1gram, pattern=tk.of.int.1gram, selection="keep")
cr.tf.1gram <- textstat_frequency(cr.dfm.1gram)
cr.tf.1gram
cr.dfm.kw <- df_lookup(cr.dfm.1gram, pattern=tk.of.int.1gram, selection="keep")
cr.dfm.kw <- df_lookup(cr.dfm.1gram, tk.of.int.1gram)
cr.dfm.kw <- dfm_lookup(cr.dfm.1gram, tk.of.int.1gram)
cr.dfm.1gram <- dfm(cr.tk.1gram)
cr.dfm.kw <- dfm_select(cr.dfm.1gram, pattern=tk.of.int.1gram, selection="keep")
cr.dfm.kw
cr.tf.1gram <- textstat_frequency(cr.dfm.kw)
cr.tf.1gram
nlra.tk.2gram <- tokens_ngrams(cr.tk.1gram, n=2)
cr.dfm.1gram <- dfm(cr.tk.2gram)
cr.tk.2gram <- tokens_ngrams(cr.tk.1gram, n=2)
cr.dfm.1gram <- dfm(cr.tk.2gram)
cr.dfm.1gram <- dfm(cr.tk.1gram)
cr.dfm.2gram <- dfm(cr.tk.2gram)
cr.corpus = corpus(cr.txt)
cr.tk.1gram <- tokens_wordstem(
tokens_select(
tokens(nlra.corpus, remove_punct=TRUE,
remove_symbols=TRUE, remove_numbers=TRUE),
pattern=en.sw, selection="remove")
)
cr.tk.2gram <- tokens_ngrams(cr.tk.1gram, n=2)
cr.dfm.1gram <- dfm(cr.tk.1gram)
cr.dfm.2gram <- dfm(cr.tk.2gram)
cr.dfm.kw.1gram <- dfm_select(cr.dfm.1gram, pattern=tk.of.int.1gram, selection="keep")
cr.tf.1gram <- textstat_frequency(cr.dfm.kw.1gram)
cr.tf.1gram
cr.dfm.kw.2gram <- dfm_select(cr.dfm.2gram, pattern=tk.of.int.2gram, selection="keep")
cr.tf.2gram <- textstat_frequency(cr.dfm.kw.2gram)
cr.tf.2gram
labor <- kwic(cr.corpus, pattern='labor', window=10)
View(labor)
labor <- as.data.frame(kwic(cr.corpus, pattern='labor', window=10))
View(labor)
labor.pre.post <- paste(labor$pre, labor$post)
labor.corpus <- corpus(labor.pre.post, text="pre_post")
labor.corpus <- corpus(labor.pre.post)
View(labor.corpus)
labor.tks <- tokens_wordstem(
tokens_select(
tokens(labor.corpus, remove_punct=TRUE,
remove_symbols=TRUE, remove_numbers=TRUE),
pattern=en.sw, selection="remove"))
labor.tf <- textstat_frequency(labor.tks)
labor.tk <- tokens_wordstem(
tokens_select(
tokens(labor.corpus, remove_punct=TRUE,
remove_symbols=TRUE, remove_numbers=TRUE),
pattern=en.sw, selection="remove"))
labor.tk.dfm = dfm(labor.tk)
labor.tf <- textstat_frequency(labor.tks)
labor.tf <- textstat_frequency(labor.tk.df)
labor.tf <- textstat_frequency(labor.tk.dfm)
labor.tf
ggplot(labor.tk.dfm[1:20, ], aes(x = reorder(feature, frequency), y = frequency)) +
geom_point() + coord_flip() + labs(x = NULL, y = "Frequency")
ggplot(labor.tf[1:20, ], aes(x = reorder(feature, frequency), y = frequency)) +
geom_point() + coord_flip() + labs(x = NULL, y = "Frequency")
################################################################################
# 4.) Sentiment Analysis
################################################################################
kwic.setiment <- dfm_lookup(labor.tk, data_dictionary_LSD2015)
################################################################################
# 4.) Sentiment Analysis
################################################################################
kwic.setiment <- dfm_lookup(labor.tk.dfm, data_dictionary_LSD2015)
View(kwic.setiment)
labor <- as.data.frame(kwic(cr.corpus, pattern='labor', window=10))
labor.pre.post <- paste(labor$pre, labor$post)
labor.corpus <- corpus(labor.pre.post)
labor.corpus
labor.tk <- tokens_wordstem(
tokens_select(
tokens(labor.pre.post, remove_punct=TRUE,
remove_symbols=TRUE, remove_numbers=TRUE),
pattern=en.sw, selection="remove"))
labor.tk.dfm = dfm(labor.tk)
labor.tf <- textstat_frequency(labor.tk.dfm)
ggplot(labor.tf[1:20, ], aes(x = reorder(feature, frequency), y = frequency)) +
geom_point() + coord_flip() + labs(x = NULL, y = "Frequency")
ggplot(labor.tf[1:20, ], aes(x = reorder(feature, frequency), y = frequency)) +
geom_point() + coord_flip() + labs(x = NULL, y = "Frequency")
################################################################################
# 4.) Sentiment Analysis
################################################################################
kwic.setiment <- dfm_lookup(labor.tk.dfm, data_dictionary_LSD2015)
View(kwic.setiment)
kwic.setiment[:, 1]
View(kwic.setiment)
################################################################################
# 4.) Sentiment Analysis
################################################################################
kwic.setiment <- as.data.frame(dfm_lookup(labor.tk.dfm, data_dictionary_LSD2015))
View(kwic.setiment)
kwic.setiment$negative
neg.sum <- sum(kwic.setiment$negative)
pos.sum <- sum(kwic.setiment$positive)
neg.sum
pos.sum
