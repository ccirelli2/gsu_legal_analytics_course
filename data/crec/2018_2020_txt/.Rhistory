)
str_view_all(
string = texts,
pattern = "(death penalty)|(Death Penalty)"
)
str_view_all(
string = texts,
pattern = "(death|Death) (penalty|Penalty)"
)
str_view_all(
string = texts,
pattern = "(d|D)eath (p|P)enalty"
)
str_view_all(
string = texts,
pattern = regex( "death penalty" )
)
str_view_all(
string = texts,
pattern = regex( "death penalty" , ignore_case = TRUE)
)
str_view_all(
string = texts,
pattern = regex( "the state" , ignore_case = TRUE)
)
str_view_all(
string = texts,
pattern = regex( "death penalty" , ignore_case = TRUE)
)
str_view_all(
string = texts,
pattern = regex( "death." , ignore_case = TRUE)
)
str_view_all(
string = texts,
pattern = regex( "death...." , ignore_case = TRUE)
)
str_view_all(
string = texts,
pattern = regex( "death.*" , ignore_case = TRUE)
)
str_view_all(
string = texts,
pattern = regex( "death.+" , ignore_case = TRUE)
)
str_view_all(
string = texts,
pattern = regex( "death.*" , ignore_case = TRUE)
)
# {minimum , }
str_view_all(
string = texts,
pattern = regex( "death.{7,}" , ignore_case = TRUE)
)
str_view_all(
string = texts,
pattern = regex( "death\\s\\w\\w\\w" , ignore_case = TRUE)
)
str_view_all(
string = texts,
pattern = regex( "death\\s\\w{5}" , ignore_case = TRUE)
)
ts individually
str_view_all(
string = texts,
pattern = regex( "\\d" , ignore_case = TRUE)
)
# Attempt to identify all digits individually
str_view_all(
string = texts,
pattern = regex( "\\d/" , ignore_case = TRUE)
)
# Attempt to identify all digits individually
str_view_all(
string = texts,
pattern = regex( "\\dd/" , ignore_case = TRUE)
)
# Attempt to identify all digits individually
str_view_all(
string = texts,
pattern = regex( "\\d./" , ignore_case = TRUE)
)
antifier
str_view_all(
string = texts,
pattern = regex( "\\d+" , ignore_case = TRUE)
)
# Add in the "/" that separates date components
str_view_all(
string = texts,
pattern = regex( "\\d+/" , ignore_case = TRUE)
)
# Write out full pattern to capture the M / D / Y of the dates
str_view_all(
string = texts,
pattern = regex( "\\d+/\\d+/\\d+" , ignore_case = TRUE)
)
str_view_all(
string = texts,
pattern = regex( "\\d+(/\\d+){2}" , ignore_case = TRUE)
)
str_view_all(
string = texts,
pattern = regex( "The State\\w" , ignore_case = TRUE)
)
str_view_all(
string = texts,
pattern = regex( "The State" , ignore_case = TRUE)
)
str_view_all(
string = texts,
pattern = regex( "The State.\\s" , ignore_case = TRUE)
)
str_view_all(
string = texts,
pattern = regex( "The State." , ignore_case = TRUE)
)
str_view_all(
string = texts,
pattern = regex( "The State.\\s" , ignore_case = TRUE)
)
str_view_all(
string = texts,
pattern = regex( "The State.\\s*" , ignore_case = TRUE)
)
str_view_all(
string = texts,
pattern = regex( "The State.\\s.*" , ignore_case = TRUE)
)
str_view_all(
string = texts,
pattern = regex( "The State.\\s*." , ignore_case = TRUE)
)
str_view_all(
string = texts,
pattern = regex( "The State.\\s+." , ignore_case = TRUE)
)
str_view_all(
string = texts,
pattern = regex( "The State.\\s." , ignore_case = TRUE)
)
str_view_all(
string = texts,
pattern = regex( "The State.\\s*" , ignore_case = TRUE)
)
str_view_all(
string = texts,
pattern = regex( "The State\\w\\s*" , ignore_case = TRUE)
)
str_view_all(
string = texts,
pattern = regex( "The State\\s\\w*" , ignore_case = TRUE)
)
str_view_all(
string = texts,
pattern = regex( "The State\\w*" , ignore_case = TRUE)
)
str_view_all(
string = texts,
pattern = regex( "The State\\s*" , ignore_case = TRUE)
)
str_view_all(
string = texts,
pattern = regex( "The State\\s+" , ignore_case = TRUE)
)
str_view_all(
string = texts,
pattern = regex( "The State\\s+\\w+" , ignore_case = TRUE)
)
str_view_all(
string = texts,
pattern = regex( "(The State\\s+\\w+){3}" , ignore_case = TRUE)
)
str_view_all(
string = texts,
pattern = regex( "(The State\\s+\\w+)" , ignore_case = TRUE)
)
str_view_all(
string = texts,
pattern = regex( "(The State\\s+\\w+)*" , ignore_case = TRUE)
)
str_view_all(
string = texts,
pattern = regex( "(The State\\s+\\w+)" , ignore_case = TRUE)
)
+#########################################
##### Part 5 - Positional arguments #####
# Cheat sheet: see "ANCHORS" and "LOOKAROUNDS"
# In some instances, you will want to include the very beginning
# or the very end of an entire string as your criterion.  This is
# where "anchors" come into play.
# This pattern simply collects the entire string ("." = any character, "*" = any number or quantity)
str_view_all(
string = texts,
pattern = regex( ".*" , ignore_case = TRUE)
)
str_view_all(
string = texts,
pattern = regex( "(The State\\s+\\w+)" , ignore_case = TRUE)
)
str_view_all(
string = texts,
pattern = regex( "(The State\\s+\\w+)+" , ignore_case = TRUE)
)
str_view_all(
string = texts,
pattern = regex( "(The State\\s+\\w+\\s+)" , ignore_case = TRUE)
)
str_view_all(
string = texts,
pattern = regex( "(The State\\s+\\w+\\s+){3}" , ignore_case = TRUE)
)
str_view_all(
string = texts,
pattern = regex( "The State(\\s+\\w+\\s+){3}" , ignore_case = TRUE)
)
str_view_all(
string = texts,
pattern = regex( "The State(\\s+\\w+\\s+)" , ignore_case = TRUE)
)
str_view_all(
string = texts,
pattern = regex( "The State(\\s+\\w+\\s+)*" , ignore_case = TRUE)
)
str_view_all(
string = texts,
pattern = regex( "The State\\s+(\\w+\\s+)*" , ignore_case = TRUE)
)
str_view_all(
string = texts,
pattern = regex( "The State\\s+(\\w+\\s+){3}" , ignore_case = TRUE)
)
str_view_all(
string = texts,
pattern = regex( ".*" , ignore_case = TRUE)
)
str_view_all(
string = texts,
pattern = regex( "\\d" , ignore_case = TRUE)
)
str_view_all(
string = texts,
pattern = regex( "^\\d" , ignore_case = TRUE)
)
str_view_all(
string = texts,
pattern = regex( "^\\d.*" , ignore_case = TRUE)
)
str_view_all(
string = texts,
pattern = regex( ".*\\d$" , ignore_case = TRUE)
)
str_view_all(
string = texts,
pattern = regex( ".*\\d\\.$" , ignore_case = TRUE)
)
str_view_all(
string = texts,
pattern = regex( "(?<=not )intend.* to seek the death penalty" , ignore_case = TRUE)
)
str_view_all(
string = texts,
pattern = regex( "(?<=not.{1,100})death penalty" , ignore_case = TRUE)
)
str_view_all(
string = texts,
pattern = regex( "(?<!not.{1,100})death penalty" , ignore_case = TRUE)
)
load("C:/Users/chris.cirelli/Desktop/repositories/gsu_legal_analytics_course/data/2018_2020_workspace2.RData")
#### PRELIMINARIES ####
library(tidyverse)
View(cr_txt)
View(cr_toks_cleaned)
help(str_extract)
library(tidyverse)
help(str_extract)
library(readtext)
library(quanteda)
library(text2vec)
library(sentimentr)
library(tidytext)
library(textdata)
library(textclean)
library(ggplot2)
dir_base = 'C:\\Users\\chris.cirelli\\Desktop\\repositories\\gsu_legal_analytics_course'
dir_data = file.path(dir_base, '\\data')
dir_crec = file.path(dir_data, '\\crec\\2018_2020_txt')
dir_hw = file.path(dir_base, '\\hw\\hw4')
# Congressional Texts
setwd(dir_crec)
cr.txt <- readtext("*.txt")
cr.txt
cr.txt <- cr.txt$text
cr.ascii <- replace_non_ascii(cr.txt, replacement="", remove.nonconverted=TRUE)
cr.corpus <- corpus(cr.ascii)
cr.corpus
cr.tk.1gram <- tokens_wordstem(
tokens_select(
tokens(cr.corpus, remove_punct=TRUE,
remove_symbols=TRUE, remove_numbers=TRUE),
pattern=en.sw, selection="remove")
)
print('hello world')
cr.tk.2gram <- tokens_ngrams(cr.tk.1gram, n=2)
cr.tk.1gram <- tokens_wordstem(
tokens_select(
tokens(cr.corpus, remove_punct=TRUE,
remove_symbols=TRUE, remove_numbers=TRUE),
pattern=en.sw, selection="remove")
)
################################################################################
# 1.) Get Tokens of Interest
################################################################################
en.sw <- stopwords("en")
cr.tk.1gram <- tokens_wordstem(
tokens_select(
tokens(cr.corpus, remove_punct=TRUE,
remove_symbols=TRUE, remove_numbers=TRUE),
pattern=en.sw, selection="remove")
)
print('hello world')
cr.tk.2gram <- tokens_ngrams(cr.tk.1gram, n=2)
# Get 1 Gram Dfm
cr.1gram.dfm <- dfm(cr.tk.1gram)
cr.tf.1gram <- textstat_frequency(cr.1gram.dfm)
head(cr.tf.1gram, n=100)
ggplot(cr.tf.1gram[1:20, ], aes(x = reorder(feature, frequency), y = frequency)) +
geom_point() + coord_flip() + labs(x = NULL, y = "Frequency")
head(cr.tf.1gram, n=200)
tk.of.int.1gram <- c('labor', 'employe', 'employ', 'board', 'servic', 'unfair',
'collect', 'concili', 'right')
# Get Bi-Gram Dfm
cr.2gram.dfm <- dfm(cr.tk.2gram)
cr.tf.2gram <- textstat_frequency(cr.2gram.dfm)
head(cr.tf.2gram, n=100)
cr.tf.2gram$feature[1:100]
ggplot(cr.tf.2gram[1:20, ], aes(x = reorder(feature, frequency), y = frequency)) +
geom_point() + coord_flip() + labs(x = NULL, y = "Frequency")
tk.of.int.2gram <- c('labor_organ', 'unfair_labor', 'labor_practic',
'repres_employe', 'labor_disput', 'collect_bargain',
'employe_employ', 'nation_labor')
cr.corpus = corpus(cr.txt)
cr.corpus = corpus(cr.txt)
cr.tk.1gram <- tokens_wordstem(
tokens_select(
tokens(cr.corpus, remove_punct=TRUE,
remove_symbols=TRUE, remove_numbers=TRUE),
pattern=en.sw, selection="remove")
)
cr.tk.2gram <- tokens_ngrams(cr.tk.1gram, n=2)
cr.dfm.1gram <- dfm(cr.tk.1gram)
cr.dfm.2gram <- dfm(cr.tk.2gram)
cr.dfm.kw.1gram <- dfm_select(cr.dfm.1gram, pattern=tk.of.int.1gram, selection="keep")
cr.dfm.kw.2gram <- dfm_select(cr.dfm.2gram, pattern=tk.of.int.2gram, selection="keep")
cr.tf.1gram <- textstat_frequency(cr.dfm.kw.1gram)
cr.tf.2gram <- textstat_frequency(cr.dfm.kw.2gram)
cr.tf.1gram
cr.tf.2gram
labor <- as.data.frame(kwic(cr.corpus, pattern='labor', window=10))
labor.pre.post <- paste(labor$pre, labor$post)
labor.corpus <- corpus(labor.pre.post)
labor.tk <- tokens_wordstem(
tokens_select(
tokens(labor.pre.post, remove_punct=TRUE,
remove_symbols=TRUE, remove_numbers=TRUE),
pattern=en.sw, selection="remove"))
labor.tk.dfm = dfm(labor.tk)
labor.tf <- textstat_frequency(labor.tk.dfm)
ggplot(labor.tf[1:20, ], aes(x = reorder(feature, frequency), y = frequency)) +
geom_point() + coord_flip() + labs(x = NULL, y = "Frequency")
labor <- as.data.frame(kwic(cr.corpus, pattern='labor', window=10))
labor.pre.post <- paste(labor$pre, labor$post)
labor.corpus <- corpus(labor.pre.post)
labor.tk <- tokens_wordstem(
tokens_select(
tokens(labor.pre.post, remove_punct=TRUE,
remove_symbols=TRUE, remove_numbers=TRUE),
pattern=en.sw, selection="remove"))
labor.pre.post <- paste(labor$pre, labor$post)
labor.corpus <- corpus(labor.pre.post)
labor.tk <- tokens_wordstem(
tokens_select(
tokens(labor.pre.post, remove_punct=TRUE,
remove_symbols=TRUE, remove_numbers=TRUE),
pattern=en.sw, selection="remove"))
labor.corpus
labor <- as.data.frame(kwic(cr.corpus, pattern='servic', window=10))
labor.pre.post <- paste(labor$pre, labor$post)
labor.corpus <- corpus(labor.pre.post)
labor.corpus
labor <- as.data.frame(kwic(cr.corpus, pattern='congres', window=10))
labor.pre.post <- paste(labor$pre, labor$post)
labor.corpus <- corpus(labor.pre.post)
labor.corpus
labor
labor <- as.data.frame(kwic(cr.corpus, pattern='prayer', window=10))
labor
prayer <- as.data.frame(kwic(cr.corpus, pattern='prayer', window=10))
prayer.pre.post <- paste(prayer$pre, prayer$post)
prayer.corpus <- corpus(prayer.pre.post)
prayer.corpus
prayer.tk <- tokens_wordstem(
tokens_select(
tokens(prayer.pre.post, remove_punct=TRUE,
remove_symbols=TRUE, remove_numbers=TRUE),
pattern=en.sw, selection="remove"))
prayer.tk.dfm = dfm(prayer.tk)
prayer.tf <- textstat_frequency(prayer.tk.dfm)
ggplot(prayer.tf[1:20, ], aes(x = reorder(feature, frequency), y = frequency)) +
geom_point() + coord_flip() + labs(x = NULL, y = "Frequency")
################################################################################
# 4.) Sentiment Analysis
################################################################################
kwic.setiment <- as.data.frame(dfm_lookup(prayer.tk.dfm, data_dictionary_LSD2015))
View(kwic.setiment)
################################################################################
# 4.) Sentiment Analysis
################################################################################
kwic.setiment <- as.data.frame(dfm_lookup(prayer.tk.dfm, data_dictionary_LSD2015))
View(kwic.setiment)
# Glove Word Embedding
glove <- GlobalVectors$new(rank = 50, x_max = 10)
# Apply model to fcm
fcm_tokens <- glove$fit_transform(fcm, n_iter = 20)
# Glove Word Embedding
glove <- GlobalVectors$new(rank = 50, x_max = 10)
# Apply model to fcm
fcm_tokens <- glove$fit_transform(cr.txt, n_iter = 20)
fcm <- fcm(ct.tk.1gram,
context = "window",
count = "weighted",
weights = 1 / (1:5),
tri = TRUE)
library(readtext)
library(quanteda)
library(text2vec)
library(sentimentr)
library(tidytext)
library(textdata)
library(textclean)
library(ggplot2)
################################################################
dir_base = 'C:\\Users\\chris.cirelli\\Desktop\\repositories\\gsu_legal_analytics_course'
dir_data = file.path(dir_base, '\\data')
dir_crec = file.path(dir_data, '\\crec\\2018_2020_txt')
dir_hw = file.path(dir_base, '\\hw\\hw4')
Texts
setwd(dir_crec)
cr.txt <- readtext("*.txt")
cr.txt <- cr.txt$text
# Congressional Texts
setwd(dir_crec)
cr.txt <- readtext("*.txt")
cr.txt <- cr.txt$text
################################################################################
# 1.) Get Tokens of Interest
################################################################################
en.sw <- stopwords("en")
cr.ascii <- replace_non_ascii(cr.txt, replacement="", remove.nonconverted=TRUE)
cr.corpus <- corpus(cr.ascii)
cr.tk.1gram <- tokens_wordstem(
tokens_select(
tokens(cr.corpus, remove_punct=TRUE,
remove_symbols=TRUE, remove_numbers=TRUE),
pattern=en.sw, selection="remove")
)
cr.corpus = corpus(cr.txt)
cr.tk.1gram <- tokens_wordstem(
tokens_select(
tokens(cr.corpus, remove_punct=TRUE,
remove_symbols=TRUE, remove_numbers=TRUE),
pattern=en.sw, selection="remove")
)
# Frequency Co-occurance Matrix
fcm <- fcm(ct.tk.1gram,
context = "window",
count = "weighted",
weights = 1 / (1:5),
tri = TRUE)
# Frequency Co-occurance Matrix
fcm <- fcm(cr.tk.1gram,
context = "window",
count = "weighted",
weights = 1 / (1:5),
tri = TRUE)
# Glove Word Embedding
glove <- GlobalVectors$new(rank = 50, x_max = 10)
# Apply model to fcm
fcm_tokens <- glove$fit_transform(cr.txt, n_iter = 20)
# Apply model to fcm
fcm_tokens <- glove$fit_transform(fcm, n_iter = 20)
fcm_tokens
# Create list of N most similar words
most_similar(fcm_tokens, "prayer")
ost_similar = function(x,y,method="cosine",N=10)
{
most_sim=head(sort(
sim2(x, x[y, , drop=FALSE],
method=method)[,1], decreasing=TRUE), N)
return(list(top_words=most_sim))
}
most_similar(fcm_tokens, "prayer")
# Create list of N most similar words
## Create function for finding list of the N most similar terms
most_similar = function(x,y,method="cosine",N=10)
{
most_sim=head(sort(
sim2(x, x[y, , drop=FALSE],
method=method)[,1], decreasing=TRUE), N)
return(list(top_words=most_sim))
}
most_similar(fcm_tokens, "prayer")
