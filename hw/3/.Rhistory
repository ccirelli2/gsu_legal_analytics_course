)
#########################################################
# Kmeans
#########################################################
fit.kmeans <- kmeans(df.pca.omit.outliers, 4, nstart=25)
str(fit.kmeans)
clusplot(df.pca.omit.outliers, fit.kmeans$cluster, color=TRUE, shade=TRUE,
labels=0, lines=0)
dim(data.pca)
path2file.1 = "C:\\Users\\chris.cirelli\\Desktop\\repositories\\data_misc\\join_core_data_ny_block_groups_surplus.csv"
data <- read.csv(path2file.1)
# Subset to Surplus = True
data <- subset(data, data$surplus == "true")
dim(data)
cols <- c('bg_id', 'ESHAVGINC', 'ESPOPDENS', 'pctHUOWNER', 'pct_25_29', 'pct_0_4', 'pct_60_64', 'Incurred', 'Earned')
data.subset <- subset(data, select = cols)
features.only <- subset(data.subset, select= c('ESHAVGINC', 'ESPOPDENS', 'pctHUOWNER', 'pct_25_29', 'pct_0_4', 'pct_60_64'))
dim(features.only)
# Cal PCA
pca.fit <- prcomp(features.only, scale. = TRUE)
# Return Data
data.pca <- pca.results$x
dim(data.pca)
df.pca <- data.frame(data.pca)
dim.df.pca
df.pca <- data.frame(data.pca)
dim(df.pca)
#########################################################
# Kmeans
#########################################################
fit.kmeans <- kmeans(df.pca, 4, nstart=25)
clusplot(df.pca.omit.outliers, fit.kmeans$cluster, color=TRUE, shade=TRUE,
labels=0, lines=0)
clusplot(df.pca, fit.kmeans$cluster, color=TRUE, shade=TRUE,
labels=0, lines=0)
fviz_cluster(fit.kmeans, data = df.pca.omit.outliers)
fviz_cluster(fit.kmeans, data = df.pca)
# Optimal num clusters
fviz_nbclust(x=df.pca, kmeans, method='wss')
dim(data.subset)
df.pca$Incurred <- data.subset$Incurred
df.pca$Earned <- data.subset$Earned
head(df.pca)
#########################################################
# Kmeans
#########################################################
fit.kmeans <- kmeans(df.pca, 4, nstart=25)
str(fit.kmeans)
fit.kmeans$cluster
length(fit.kmeans$cluster)
# Add Back Earned & Incurred
df.pca$Cluster <- fit.kmeans$cluster
df.pca$Incurred <- data.subset$Incurred
df.pca$Earned <- data.subset$Earned
head(df.pca)
df.grouped <- df.pca %>%
group_by(df.pca, Cluster) %>%
summarise(sum = sum(Incurred))
df.grouped <- df.pca %>%
group_by(Cluster) %>%
summarise(sum = sum(Incurred))
df.pca %>% group_by(Cluster)
df.pca %>% group_by(Cluster)%>% summarise( sum_incurred = sum(Incurred),
sum_earned = sum(Earned))
df.grouped <- df.pca %>% group_by(Cluster)%>% summarise( sum_incurred = sum(Incurred),
sum_earned = sum(Earned))
df.grouped
df.grouped$LossRatio <- df.grouped$sum_incurred / df.grouped$sum_earned
df.grouped
df.grouped <- df.pca %>% group_by(Cluster)%>% summarise( cnt = count(Cluster), sum_incurred = sum(Incurred),
sum_earned = sum(Earned))
df.grouped <- df.pca %>% group_by(Cluster)%>% summarise( cnt = cnt(Cluster), sum_incurred = sum(Incurred),
sum_earned = sum(Earned))
df.grouped <- df.pca %>% group_by(Cluster)%>% summarise( cnt = n(Cluster), sum_incurred = sum(Incurred),
sum_earned = sum(Earned))
df.grouped <- df.pca %>% group_by(Cluster)%>% summarise(cnt = n(df.pca$PC2), sum_incurred = sum(Incurred),
sum_earned = sum(Earned))
df.grouped <- df.pca %>% group_by(Cluster)%>% summarise(sum_incurred = sum(Incurred),
sum_earned = sum(Earned))
df.grouped$LossRatio <- df.grouped$sum_incurred / df.grouped$sum_earned
df.grouped
df.grouped <- df.pca %>% group_by(Cluster)%>% summarise(sum_incurred = sum(Incurred),
sum_earned = sum(Earned),
cnt = n())
df.grouped$LossRatio <- df.grouped$sum_incurred / df.grouped$sum_earned
df.grouped
#########################################################
# Kmeans
#########################################################
fit.kmeans <- kmeans(df.pca, 6, nstart=25)
# Add Back Earned & Incurred
df.pca$Cluster <- fit.kmeans$cluster
df.pca$Incurred <- data.subset$Incurred
df.pca$Earned <- data.subset$Earned
df.grouped <- df.pca %>% group_by(Cluster)%>% summarise(sum_incurred = sum(Incurred),
sum_earned = sum(Earned),
cnt = n())
df.grouped$LossRatio <- df.grouped$sum_incurred / df.grouped$sum_earned
df.grouped
path2file.1 = "C:\\Users\\chris.cirelli\\Desktop\\repositories\\data_misc\\join_core_data_ny_block_groups_surplus.csv"
data <- read.csv(path2file.1)
# Subset to Surplus = True
data <- subset(data, data$surplus == "true")
dim(data)
cols <- c('bg_id', 'ESHAVGINC', 'ESPOPDENS', 'pctHUOWNER', 'pct_25_29', 'pct_0_4', 'pct_60_64', 'Incurred', 'Earned')
data.subset <- subset(data, select = cols)
features.only <- subset(data.subset, select= c('ESHAVGINC', 'ESPOPDENS', 'pctHUOWNER', 'pct_25_29', 'pct_0_4', 'pct_60_64'))
dim(features.only)
# Cal PCA
pca.fit <- prcomp(features.only, scale. = TRUE)
fviz_eig(pca.fit)
# Get Summary
pca.results <- summary(pca.fit)
pca.cdf <- pca.results$importance[3, 1:6]
pca.cdf
df.pca <- subset(df.pca, select = -c(PC3, PC4, PC5, PC6))
#########################################################
# Kmeans
#########################################################
fit.kmeans <- kmeans(df.pca, 6, nstart=25)
# Add Back Earned & Incurred
df.pca$Cluster <- fit.kmeans$cluster
df.pca$Incurred <- data.subset$Incurred
df.pca$Earned <- data.subset$Earned
df.grouped <- df.pca %>% group_by(Cluster)%>% summarise(sum_incurred = sum(Incurred),
sum_earned = sum(Earned),
cnt = n())
df.grouped$LossRatio <- df.grouped$sum_incurred / df.grouped$sum_earned
df.grouped
#########################################################
# Kmeans
#########################################################
fit.kmeans <- kmeans(df.pca, 3, nstart=25)
clusplot(df.pca, fit.kmeans$cluster, color=TRUE, shade=TRUE,
labels=0, lines=0)
fviz_cluster(fit.kmeans, data = df.pca)
df.grouped <- df.pca %>% group_by(Cluster)%>% summarise(sum_incurred = sum(Incurred),
sum_earned = sum(Earned),
cnt = n())
# Add Back Earned & Incurred
df.pca$Cluster <- fit.kmeans$cluster
df.pca$Incurred <- data.subset$Incurred
df.pca$Earned <- data.subset$Earned
df.grouped <- df.pca %>% group_by(Cluster)%>% summarise(sum_incurred = sum(Incurred),
sum_earned = sum(Earned),
cnt = n())
df.grouped$LossRatio <- df.grouped$sum_incurred / df.grouped$sum_earned
df.grouped
# Return Data
data.pca <- pca.results$x
dim(data.pca)
df.pca <- data.frame(data.pca)
fit.kmeans <- kmeans(df.pca, 3, nstart=25)
clusplot(df.pca, fit.kmeans$cluster, color=TRUE, shade=TRUE,
labels=0, lines=0)
fviz_cluster(fit.kmeans, data = df.pca)
# Optimal num clusters
fviz_nbclust(x=df.pca, kmeans, method='wss')
# Add Back Earned & Incurred
df.pca$Cluster <- fit.kmeans$cluster
df.pca$Incurred <- data.subset$Incurred
df.pca$Earned <- data.subset$Earned
df.grouped <- df.pca %>% group_by(Cluster)%>% summarise(sum_incurred = sum(Incurred),
sum_earned = sum(Earned),
cnt = n())
df.grouped$LossRatio <- df.grouped$sum_incurred / df.grouped$sum_earned
df.grouped
###########################
fit.kmeans <- kmeans(df.pca, 6, nstart=25)
'
clusplot(df.pca, fit.kmeans$cluster, color=TRUE, shade=TRUE,
labels=0, lines=0)
fviz_cluster(fit.kmeans, data = df.pca)
# Optimal num clusters
fviz_nbclust(x=df.pca, kmeans, method='wss')
'
# Add Back Earned & Incurred
df.pca$Cluster <- fit.kmeans$cluster
df.pca$Incurred <- data.subset$Incurred
df.pca$Earned <- data.subset$Earned
df.grouped <- df.pca %>% group_by(Cluster)%>% summarise(sum_incurred = sum(Incurred),
sum_earned = sum(Earned),
cnt = n())
df.grouped$LossRatio <- df.grouped$sum_incurred / df.grouped$sum_earned
df.grouped
distance <- get_dist(features.only)
fviz_dist(distance, gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"))
#########################################################
# Kmeans
#########################################################
fit.kmeans <- kmeans(df.pca, 6, nstart=25)
"
clusplot(df.pca, fit.kmeans$cluster, color=TRUE, shade=TRUE,
labels=0, lines=0)
fviz_cluster(fit.kmeans, data = df.pca)
# Optimal num clusters
fviz_nbclust(x=df.pca, kmeans, method='wss')
"
# Add Back Earned & Incurred
df.pca$Cluster <- fit.kmeans$cluster
df.pca$Incurred <- data.subset$Incurred
df.pca$Earned <- data.subset$Earned
df.grouped <- df.pca %>% group_by(Cluster)%>% summarise(sum_incurred = sum(Incurred),
sum_earned = sum(Earned),
cnt = n())
df.grouped$LossRatio <- df.grouped$sum_incurred / df.grouped$sum_earned
df.grouped
sdfds
#########################################################
# Kmeans
#########################################################
fit.kmeans <- kmeans(df.pca, 6, nstart=25)
# Add Back Earned & Incurred
df.pca$Cluster <- fit.kmeans$cluster
df.pca$Incurred <- data.subset$Incurred
df.pca$Earned <- data.subset$Earned
df.grouped <- df.pca %>% group_by(Cluster)%>% summarise(sum_incurred = sum(Incurred),
sum_earned = sum(Earned),
cnt = n())
df.grouped$LossRatio <- df.grouped$sum_incurred / df.grouped$sum_earned
df.grouped
print('test')
##################################
# Load Libraries
#########################################################
library(ggplot2)
library(ggfortify)
library(cluster)
library(fpc)
library(tidyverse)
library(factoextra)
Ref <- 'https://uc-r.github.io/kmeans_clustering'
Ref2 <- 'http://www.sthda.com/english/articles/31-principal-component-methods-in-r-practical-guide/118-principal-component-analysis-in-r-prcomp-vs-princomp/'
#########################################################
# Load Data
#########################################################
path2file.1 = "C:\\Users\\chris.cirelli\\Desktop\\repositories\\data_misc\\join_core_data_ny_block_groups_surplus.csv"
data <- read.csv(path2file.1)
# Subset to Surplus = True
data <- subset(data, data$surplus == "true")
dim(data)
cols <- c('bg_id', 'ESHAVGINC', 'ESPOPDENS', 'pctHUOWNER', 'pct_25_29', 'pct_0_4', 'pct_60_64', 'Incurred', 'Earned')
data.subset <- subset(data, select = cols)
features.only <- subset(data.subset, select= c('ESHAVGINC', 'ESPOPDENS', 'pctHUOWNER', 'pct_25_29', 'pct_0_4', 'pct_60_64'))
dim(features.only)
#########################################################
# Get Principal Components
#########################################################
# Cal PCA
pca.fit <- prcomp(features.only, scale. = TRUE)
pca.results <- summary(pca.fit)
pca.cdf <- pca.results$importance[3, 1:6]
pca.cdf
plot(pca.cdf)
# Return Data
data.pca <- pca.results$x
dim(data.pca)
df.pca <- data.frame(data.pca)
#########################################################
# Kmeans
#########################################################
fit.kmeans <- kmeans(df.pca, 6, nstart=25)
# Add Back Earned & Incurred
df.pca$Cluster <- fit.kmeans$cluster
df.pca$Incurred <- data.subset$Incurred
df.pca$Earned <- data.subset$Earned
df.grouped <- df.pca %>% group_by(Cluster)%>% summarise(sum_incurred = sum(Incurred),
sum_earned = sum(Earned),
cnt = n())
df.grouped$LossRatio <- df.grouped$sum_incurred / df.grouped$sum_earned
df.grouped
df.grouped$PctNum <- df.grouped$cnt / sum(df.grouped$cnt)
df.grouped
df.grouped$PctNum <- df.grouped$cnt / sum(df.grouped$cnt) * 100
df.grouped
clusplot(df.pca, fit.kmeans$cluster, color=TRUE, shade=TRUE,
labels=0, lines=0)
fviz_cluster(fit.kmeans, data = df.pca)
#########################################################
# Kmeans
#########################################################
fit.kmeans <- kmeans(df.pca, 4, nstart=25)
fviz_cluster(fit.kmeans, data = df.pca)
fviz_cluster(fit.kmeans, data = df.pca)
clusplot(df.pca, fit.kmeans$cluster, color=TRUE, shade=TRUE,
labels=0, lines=0)
#########################################################
# Kmeans
#########################################################
fit.kmeans <- kmeans(df.pca, 4, nstart=25)
df.pca$Cluster <- fit.kmeans$cluster
df.pca$Incurred <- data.subset$Incurred
df.pca$Earned <- data.subset$Earned
df.grouped <- df.pca %>% group_by(Cluster)%>% summarise(sum_incurred = sum(Incurred),
sum_earned = sum(Earned),
cnt = n())
df.grouped$LossRatio <- df.grouped$sum_incurred / df.grouped$sum_earned
df.grouped$PctNum <- df.grouped$cnt / sum(df.grouped$cnt) * 100
df.grouped
#########################################################
# Kmeans
#########################################################
fit.kmeans <- kmeans(df.pca, 10, nstart=25)
clusplot(df.pca, fit.kmeans$cluster, color=TRUE, shade=TRUE,
labels=0, lines=0)
fviz_cluster(fit.kmeans, data = df.pca)
df.pca$Cluster <- fit.kmeans$cluster
df.pca$Incurred <- data.subset$Incurred
df.pca$Earned <- data.subset$Earned
df.grouped <- df.pca %>% group_by(Cluster)%>% summarise(sum_incurred = sum(Incurred),
sum_earned = sum(Earned),
cnt = n())
df.grouped$LossRatio <- df.grouped$sum_incurred / df.grouped$sum_earned
df.grouped$PctNum <- df.grouped$cnt / sum(df.grouped$cnt) * 100
df.grouped
#########################################################
# Kmeans
#########################################################
fit.kmeans <- kmeans(df.pca, 3, nstart=25)
clusplot(df.pca, fit.kmeans$cluster, color=TRUE, shade=TRUE,
labels=0, lines=0)
df.pca <- subset(df.pca, select = -c(PC3, PC4, PC5, PC6))
#########################################################
# Kmeans
#########################################################
fit.kmeans <- kmeans(df.pca, 3, nstart=25)
clusplot(df.pca, fit.kmeans$cluster, color=TRUE, shade=TRUE,
labels=0, lines=0)
#########################################################
# Kmeans
#########################################################
fit.kmeans <- kmeans(df.pca, 4, nstart=25)
clusplot(df.pca, fit.kmeans$cluster, color=TRUE, shade=TRUE,
labels=0, lines=0)
#########################################################
# Kmeans
#########################################################
fit.kmeans <- kmeans(df.pca, 6, nstart=25)
clusplot(df.pca, fit.kmeans$cluster, color=TRUE, shade=TRUE,
labels=0, lines=0)
fviz_cluster(fit.kmeans, data = df.pca)
# Add Back Earned & Incurred
df.pca$Cluster <- fit.kmeans$cluster
df.pca$Incurred <- data.subset$Incurred
df.pca$Earned <- data.subset$Earned
df.grouped <- df.pca %>% group_by(Cluster)%>% summarise(sum_incurred = sum(Incurred),
sum_earned = sum(Earned),
cnt = n())
df.grouped$LossRatio <- df.grouped$sum_incurred / df.grouped$sum_earned
df.grouped$PctNum <- df.grouped$cnt / sum(df.grouped$cnt) * 100
df.grouped
library(data.table)
DT = data.table(
ID = c("b","b","b","a","a","c"),
a = 1:6,
b = 7:12,
c = 13:18)
DT
dt<- data.table(
id = c("b","b","b","a","a","c"),
a = 1:6,
b = 7:12,
c = 13:18)
dt
dt[, surplus := TRUE]
dt
devtools::install_github("jakobbossek/ecr2")
pkgbuild::check_build_tools(debug = TRUE)
devtools::install_github("jakobbossek/ecr")
install.library(devtools)
install.packages("devtools")
install.packages("renv")
renv::install("jakobbossek/ecr2")
library(ecr)
# Find Global Minima of Ackley-Funciton
fn = makeAckleyFunction(1L)
autoplot(fn, show.optimum=TRUE, length.out = 1000)
library(ggplot2)
library(smoof)
# Find Global Minima of Ackley-Funciton
fn = makeAckleyFunction(1L)
autoplot(fn, show.optimum=TRUE, length.out = 1000)
# Frequency plot
# Assign your textstat_frequency results to an object
tstat <- textstat_frequency(cr_dfm_clean)
ggplot(tstat[1:50, ], aes(x = reorder(feature, frequency), y = frequency)) +
geom_point() +
coord_flip() +
labs(x = NULL, y = "Frequency")
############################################################################
library(quanteda)
library(ggplot2)
# Freq Plot
ggplot(tstat[1:50, ], aes(x = reorder(feature, frequency), y = frequency)) +
geom_point() +
coord_flip() +
labs(x = NULL, y = "Frequency")
# Frequency plot
# Assign your textstat_frequency results to an object
tstat <- textstat_frequency(cr_dfm_clean)
# Frequency plot
# Assign your textstat_frequency results to an object
tstat <- textstat_frequency(cr_dfm_clean)
############################################################################
# QUESTIONS
############################################################################
# 1) How many documents are in this corpus
cr_dfm_clean
summary(cr_dfm_clean)
head(cr_dfm_clean)
'Answer : 636 Documents
'
# 2) What are the top 10 tokens and their frequency?
topfeatures(cr_dfm_clean)
head(textstat_frequency(cr_dfm_clean), n=10)
'Answer :
feature frequency
1       mr    517849
2        s    294143
3      act    245887
4  section    226094
5   states    203579
6    shall    199942
7   united    189683
8    house    183585
9        b    180166
10    bill    157303
'
# 3) From your review of the frequency table, what are the additional
#    stopwords that you would liek to drop from the 2018 to 2020 file?
# Load Stopwords
stopwords.en <- stopwords('en')
stopwords.en = stopwords.en[-c(which(stopwords.en=="i"))]
n_toks = 100
tokens <- textstat_frequency(cr_dfm_clean)[1:n_toks, 1]
tokens
'mr, s, b, h.r, ms, re-, e, c, f, con-, in-, ii, sep, d, d-, com-, pro-, fm,
use, us,
'
# 4) Generate a wordcloud and textstat_frequency plot to show top 50 words
# Frequency plot
# Assign your textstat_frequency results to an object
tstat <- textstat_frequency(cr_dfm_clean)
# Freq Plot
ggplot(tstat[1:50, ], aes(x = reorder(feature, frequency), y = frequency)) +
geom_point() +
coord_flip() +
labs(x = NULL, y = "Frequency")
# Frequency plot
# Assign your textstat_frequency results to an object
tstat <- textstat_frequency(cr_dfm_clean)
# 1) How many documents are in this corpus
cr_dfm_clean
# Set Directories
############################################################################
setwd('C:\\Users\\chris.cirelli\\Desktop\\repositories\\gsu_legal_analytics_course\\hw\\hw3')
############################################################################
# Load Data
############################################################################
load('2018_2020_workspace.RData')
# 1) How many documents are in this corpus
cr_dfm_clean
summary(cr_dfm_clean)
head(cr_dfm_clean)
'Answer : 636 Documents
'
# 2) What are the top 10 tokens and their frequency?
topfeatures(cr_dfm_clean)
head(textstat_frequency(cr_dfm_clean), n=10)
'Answer :
feature frequency
1       mr    517849
2        s    294143
3      act    245887
4  section    226094
5   states    203579
6    shall    199942
7   united    189683
8    house    183585
9        b    180166
10    bill    157303
'
# 3) From your review of the frequency table, what are the additional
#    stopwords that you would liek to drop from the 2018 to 2020 file?
# Load Stopwords
stopwords.en <- stopwords('en')
stopwords.en = stopwords.en[-c(which(stopwords.en=="i"))]
n_toks = 100
tokens <- textstat_frequency(cr_dfm_clean)[1:n_toks, 1]
tokens
'mr, s, b, h.r, ms, re-, e, c, f, con-, in-, ii, sep, d, d-, com-, pro-, fm,
use, us,
'
# 4) Generate a wordcloud and textstat_frequency plot to show top 50 words
# Frequency plot
# Assign your textstat_frequency results to an object
tstat <- textstat_frequency(cr_dfm)
# Freq Plot
ggplot(tstat[1:50, ], aes(x = reorder(feature, frequency), y = frequency)) +
geom_point() +
coord_flip() +
labs(x = NULL, y = "Frequency")
# Frequency plot
# Assign your textstat_frequency results to an object
tstat <- textstat_frequency(cr_dfm_clean)
ggplot(tstat[1:50, ], aes(x = reorder(feature, frequency), y = frequency)) +
geom_point() +
coord_flip() +
labs(x = NULL, y = "Frequency")
# Generate the word cloud
dev.new(width = 1000, height = 1000, unit = "px")
textplot_wordcloud(cr_dfm_clean, max_words = 50)
