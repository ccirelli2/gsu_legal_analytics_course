pca.cdf
plot(pca.cdf)
# Return Data
data.pca <- pca.results$x
dim(data.pca)
df.pca <- data.frame(data.pca)
#########################################################
# Kmeans
#########################################################
fit.kmeans <- kmeans(df.pca, 6, nstart=25)
# Add Back Earned & Incurred
df.pca$Cluster <- fit.kmeans$cluster
df.pca$Incurred <- data.subset$Incurred
df.pca$Earned <- data.subset$Earned
df.grouped <- df.pca %>% group_by(Cluster)%>% summarise(sum_incurred = sum(Incurred),
sum_earned = sum(Earned),
cnt = n())
df.grouped$LossRatio <- df.grouped$sum_incurred / df.grouped$sum_earned
df.grouped
df.grouped$PctNum <- df.grouped$cnt / sum(df.grouped$cnt)
df.grouped
df.grouped$PctNum <- df.grouped$cnt / sum(df.grouped$cnt) * 100
df.grouped
clusplot(df.pca, fit.kmeans$cluster, color=TRUE, shade=TRUE,
labels=0, lines=0)
fviz_cluster(fit.kmeans, data = df.pca)
#########################################################
# Kmeans
#########################################################
fit.kmeans <- kmeans(df.pca, 4, nstart=25)
fviz_cluster(fit.kmeans, data = df.pca)
fviz_cluster(fit.kmeans, data = df.pca)
clusplot(df.pca, fit.kmeans$cluster, color=TRUE, shade=TRUE,
labels=0, lines=0)
#########################################################
# Kmeans
#########################################################
fit.kmeans <- kmeans(df.pca, 4, nstart=25)
df.pca$Cluster <- fit.kmeans$cluster
df.pca$Incurred <- data.subset$Incurred
df.pca$Earned <- data.subset$Earned
df.grouped <- df.pca %>% group_by(Cluster)%>% summarise(sum_incurred = sum(Incurred),
sum_earned = sum(Earned),
cnt = n())
df.grouped$LossRatio <- df.grouped$sum_incurred / df.grouped$sum_earned
df.grouped$PctNum <- df.grouped$cnt / sum(df.grouped$cnt) * 100
df.grouped
#########################################################
# Kmeans
#########################################################
fit.kmeans <- kmeans(df.pca, 10, nstart=25)
clusplot(df.pca, fit.kmeans$cluster, color=TRUE, shade=TRUE,
labels=0, lines=0)
fviz_cluster(fit.kmeans, data = df.pca)
df.pca$Cluster <- fit.kmeans$cluster
df.pca$Incurred <- data.subset$Incurred
df.pca$Earned <- data.subset$Earned
df.grouped <- df.pca %>% group_by(Cluster)%>% summarise(sum_incurred = sum(Incurred),
sum_earned = sum(Earned),
cnt = n())
df.grouped$LossRatio <- df.grouped$sum_incurred / df.grouped$sum_earned
df.grouped$PctNum <- df.grouped$cnt / sum(df.grouped$cnt) * 100
df.grouped
#########################################################
# Kmeans
#########################################################
fit.kmeans <- kmeans(df.pca, 3, nstart=25)
clusplot(df.pca, fit.kmeans$cluster, color=TRUE, shade=TRUE,
labels=0, lines=0)
df.pca <- subset(df.pca, select = -c(PC3, PC4, PC5, PC6))
#########################################################
# Kmeans
#########################################################
fit.kmeans <- kmeans(df.pca, 3, nstart=25)
clusplot(df.pca, fit.kmeans$cluster, color=TRUE, shade=TRUE,
labels=0, lines=0)
#########################################################
# Kmeans
#########################################################
fit.kmeans <- kmeans(df.pca, 4, nstart=25)
clusplot(df.pca, fit.kmeans$cluster, color=TRUE, shade=TRUE,
labels=0, lines=0)
#########################################################
# Kmeans
#########################################################
fit.kmeans <- kmeans(df.pca, 6, nstart=25)
clusplot(df.pca, fit.kmeans$cluster, color=TRUE, shade=TRUE,
labels=0, lines=0)
fviz_cluster(fit.kmeans, data = df.pca)
# Add Back Earned & Incurred
df.pca$Cluster <- fit.kmeans$cluster
df.pca$Incurred <- data.subset$Incurred
df.pca$Earned <- data.subset$Earned
df.grouped <- df.pca %>% group_by(Cluster)%>% summarise(sum_incurred = sum(Incurred),
sum_earned = sum(Earned),
cnt = n())
df.grouped$LossRatio <- df.grouped$sum_incurred / df.grouped$sum_earned
df.grouped$PctNum <- df.grouped$cnt / sum(df.grouped$cnt) * 100
df.grouped
library(data.table)
DT = data.table(
ID = c("b","b","b","a","a","c"),
a = 1:6,
b = 7:12,
c = 13:18)
DT
dt<- data.table(
id = c("b","b","b","a","a","c"),
a = 1:6,
b = 7:12,
c = 13:18)
dt
dt[, surplus := TRUE]
dt
devtools::install_github("jakobbossek/ecr2")
pkgbuild::check_build_tools(debug = TRUE)
devtools::install_github("jakobbossek/ecr")
install.library(devtools)
install.packages("devtools")
install.packages("renv")
renv::install("jakobbossek/ecr2")
library(ecr)
# Find Global Minima of Ackley-Funciton
fn = makeAckleyFunction(1L)
autoplot(fn, show.optimum=TRUE, length.out = 1000)
library(ggplot2)
library(smoof)
# Find Global Minima of Ackley-Funciton
fn = makeAckleyFunction(1L)
autoplot(fn, show.optimum=TRUE, length.out = 1000)
# Load the relevant libraries
library(readtext)
library(quanteda)
library(text2vec)
library(sentimentr)
library(tidytext)
library(textdata)
# Alternatively, set your working directory to the location of the
# 10_txt folder on your machine.
setwd("C:\\Users\\chris.cirelli\\Desktop\\repositories\\gsu_legal_analytics_course\\data")
# Read in your dataset; create corpus, tokens, and dfm objects
# Note that the functions below take different objects as inputs: tokens, corpus, dfm
cr_txt <- readtext("*.txt")
cr_corpus <- corpus(cr_txt)
cr_toks <- tokens(cr_corpus)
cr_dfm <- dfm(cr_corpus)
summary(cr_dfm)
cossim = function(x,y)
{
numerator = sqrt(sum(x*y))
denominator = sqrt(sum(x^2)) * sqrt(sum(y^2))
return(list(cosine_similarity = numerator/denominator))
}
## Create function for finding list of the N most similar terms
most_similar = function(x,y,method="cosine",N=10)
{
most_sim=head(sort(
sim2(x, x[y, , drop=FALSE],
method=method)[,1], decreasing=TRUE), N)
return(list(top_words=most_sim))
}
# Create a feature-co-occurrence matrix (fcm) using the tokens object
fcm <- fcm(cr_toks,
context = "window",
count = "weighted",
weights = 1 / (1:5),
tri = TRUE)
fcm
# Set up and fit the GloVe model
glove <- GlobalVectors$new(rank = 50, x_max = 10)
# Apply model to fcm
fcm_tokens <- glove$fit_transform(fcm, n_iter = 20)
# Create list of N most similar words
most_similar(fcm_tokens, "economic")
# Create list of N most similar words
most_similar(fcm_tokens, "man")
# Create list of N most similar words
most_similar(fcm_tokens, "women")
fcm
help(fcm)
# Set up and fit the GloVe model
glove <- GlobalVectors$new(rank = 50, x_max = 10)
glove
# Apply model to fcm
fcm_tokens <- glove$fit_transform(fcm, n_iter = 20)
# Create list of N most similar words
most_similar(fcm_tokens, "women")
# Create list of N most similar words
most_similar(fcm_tokens, "shit")
# Create list of N most similar words
most_similar(fcm_tokens, "congress")
# Create list of N most similar words
most_similar(fcm_tokens, "congressmen")
fcm_tokens
# Create list of N most similar words
most_similar(fcm_tokens, "debates")
# Create list of N most similar words
most_similar(fcm_tokens, "america")
# Create list of N most similar words
most_similar(fcm_tokens, "AMERICA")
# Create list of N most similar words
most_similar(fcm_tokens, "PROCEEDINGS")
# Using the dfm you created above, isolate your list of keywords
dfm_keywords <- dfm_select(cr_dfm, pattern = c("worker","taxpayer","consumer"),
selection = "keep")
View(dfm_keywords)
# Write out to a csv file if you like
dfm_keywords <- convert(dfm_keywords, to = "data.frame")
write.table(dfm_keywords, "dfm_keywords.csv")
# Alternative: read in list of keywords from txt document
# Use dfm_select on the basis of that object
keywords <- readLines("keywords.txt")
# Alternative: read in list of keywords from txt document
# Use dfm_select on the basis of that object
keywords <- readLines("dfm_keywords.txt")
# Write out to a csv file if you like
dfm_keywords <- convert(dfm_keywords, to = "data.frame")
write.table(dfm_keywords, "dfm_keywords.csv")
dict <- dictionary(list(worker = c("worker", "union", "employee"),
consumer = c("consumer", "prices", "competition"),
taxglob = "tax*"))
dfm_keywords <- dfm_lookup(cr_dfm, dict, valuetype = "glob") # Specify glob because one dictionary item is in glob format
View(dfm_keywords) # Note that this sums all dictionary word counts
# Recreate your dfm from your toks object
cr_dfm2 <- dfm(cr_toks2)
cr_toks2 <- tokens_replace(cr_toks, phrase(c("minimum wage")),
phrase(c("minimum_wage")))
cr_toks2
cr_dfm2
# Recreate your dfm from your toks object
cr_dfm2 <- dfm(cr_toks2)
cr_dfm2
write.table(dfm_keywords, "dfm_keywords.csv")
# Write out to a csv file if you like
dfm_keywords <- convert(dfm_keywords, to = "data.frame")
write.table(dfm_keywords, "dfm_keywords.csv")
# Alternative: read in list of keywords from txt document
# Use dfm_select on the basis of that object
keywords <- readLines("dfm_keywords.txt")
# Alternative: read in list of keywords from txt document
# Use dfm_select on the basis of that object
keywords <- readLines("dfm_keywords.txt")
# Alternative: read in list of keywords from txt document
# Use dfm_select on the basis of that object
keywords <- readLines("dfm_keywords.txt")
dict <- dictionary(list(worker = c("worker", "union", "employee"),
consumer = c("consumer", "prices", "competition"),
taxglob = "tax*"))
dfm_keywords <- dfm_lookup(cr_dfm, dict, valuetype = "glob") # Specify glob because one dictionary item is in glob format
dfm_keywords)
dfm_keywords
View(dfm_keywords) # Note that this sums all dictionary word counts
dfm_keywords <- dfm_lookup(cr_dfm, dict, valuetype = "glob",
case_insensitive = FALSE) # Specify glob because one dictionary item is in glob format
dfm_keywords
cr_toks2 <- tokens_replace(cr_toks, phrase(c("minimum wage")),
phrase(c("minimum_wage")))
cr_toks2
cr_dfm2 <- dfm(cr_toks2)
cr_dfm2
View(dfm_keywords2)
# Add those bigrams/ngrams to your dfm_select or dfm_lookup workflow
dfm_keywords2 <- dfm_select(cr_dfm2, pattern = c("minimum_wage","economic","worker","taxpayer","consumer"),
selection = "keep")
View(dfm_keywords2)
source('C:/Users/chris.cirelli/Desktop/repositories/gsu_legal_analytics_course/week4/week 4 code.R', echo=TRUE)
# Turn your kwic object into a dataframe (key words in context)
economic <- as.data.frame(economic)
# Look at it
View(economic)
# Turn your kwic object into a dataframe
econ <- as.data.frame(econ)
econ <- kwic(cr_corpus, pattern = "econ*",
window = 10, valuetype = "glob")
# Turn your kwic object into a dataframe
econ <- as.data.frame(econ)
# Look at it
View(econ)
minimum_wage <- kwic(cr_corpus, pattern = phrase("minimum wage"),
window = 10, valuetype = "glob")
# Turn your kwic object into a data.frame
minimum_wage <- as.data.frame(minimum_wage)
economic <- kwic(cr_corpus, pattern = "economic",
window = 10, valuetype = "fixed")
# Turn your kwic object into a dataframe (key words in context)
economic <- as.data.frame(economic)
# Look at it
View(economic)
# Phrase look-up
minimum_wage <- kwic(cr_corpus, pattern = phrase("minimum wage"),
window = 10)      # case insensitive
# Turn your kwic object into a data.frame
minimum_wage <- as.data.frame(minimum_wage)
# Look at it
View(minimum_wage)
View(kwic_read)
# Look at it
View(minimum_wage)
View(dfm_keywords)
dfm_keywords <- convert(dfm_keywords, to = "data.frame")
write.table(dfm_keywords, "dfm_keywords.csv")
# Using the dfm you created above, isolate your list of keywords
dfm_keywords <- dfm_select(cr_dfm, pattern = c("worker","taxpayer","consumer"),
selection = "keep")
View(dfm_keywords)
# Write out to a csv file if you like
dfm_keywords <- convert(dfm_keywords, to = "data.frame")
write.table(dfm_keywords, "dfm_keywords.csv")
# Alternative: read in list of keywords from txt document
# Use dfm_select on the basis of that object
keywords <- readLines("dfm_keywords.txt")
# Alternative: read in list of keywords from txt document
# Use dfm_select on the basis of that object
keywords <- readLines("dfm_keywords.csv")
dfm_keywords <- dfm_select(cr_dfm, pattern = keywords,
selection = "keep")
# Another alternative: create a dictionary object to group multiple words under a single label
# Then use dfm_lookup
# https://www.rdocumentation.org/packages/quanteda/versions/2.1.2/topics/dfm_lookup
worker
dict <- dictionary(list(worker = c("worker", "union", "employee"),
consumer = c("consumer", "prices", "competition"),
taxglob = "tax*"))
dict
# Pass the dictionary to dfm_lookup to build your dfm
dfm_keywords <- dfm_lookup(cr_dfm, dict, valuetype = "glob",
case_insensitive = FALSE) # Specify glob because one dictionary item is in glob format
dfm_keywords
dfm_keywords
View(dfm_keywords) # Note that this sums all dictionary word counts
cr_toks2 <- tokens_replace(cr_toks, phrase(c("minimum wage")),
phrase(c("minimum_wage")))
cr_toks2
view(cr_toks2)
View(cr_toks2)
typeof(dfm_keywords2)
summary(dfm_keywords2)
dfm_keywords2 <- dfm_select(cr_dfm2, pattern = c("minimum_wage","economic","worker","taxpayer","consumer"),
selection = "keep")
View(dfm_keywords2)
economic <- kwic(cr_corpus, pattern = "economic",
window = 10, valuetype = "fixed")
economic
View(economic)
# Turn your kwic object into a dataframe (key words in context)
economic <- as.data.frame(economic)
# Look at it
View(economic)
econ <- kwic(cr_corpus, pattern = "econ*",
window = 10, valuetype = "glob")
# Turn your kwic object into a dataframe
econ <- as.data.frame(econ)
# Look at it
View(econ)
# Turn your kwic object into a dataframe (key words in context)
economic <- as.data.frame(economic)
# Look at it
View(economic)
minimum_wage <- kwic(cr_corpus, pattern = phrase("minimum wage"),
window = 10)      # case insensitive
minimum_wage
minimum_wage$pre_post <- paste(minimum_wage$pre, minimum_wage$post)
View(minimum_wage)
minimum_wage <- as.data.frame(kwic(cr_corpus, pattern = phrase("minimum wage"),
window = 10))
# Combine pre- and post- text into a single column in a new dataframe
minimum_wage$pre_post <- paste(minimum_wage$pre, minimum_wage$post)
View(minimum_wage)
# Combine pre- and post- text into a single column in a new dataframe
minimum_wage$pre_post <- paste(minimum_wage$pre, minimum_wage$post)
View(minimum_wage)
# Turn it into a corpus; tell the corpus command where the text is
# Note: each row is a kwic extract, not a Cong. Rec. document
kwic_corpus <- corpus(minimum_wage, text = "pre_post")
head(corpus)
View(kwic_corpus)
# Tokenize
kwic_toks <- tokens(kwic_corpus)
View(kwic_toks)
# Look at it
View(minimum_wage)
# Combine pre- and post- text into a single column in a new dataframe
minimum_wage$pre_post <- paste(minimum_wage$pre, minimum_wage$post)
View(minimum_wage)
# Turn it into a corpus; tell the corpus command where the text is
# Note: each row is a kwic extract, not a Cong. Rec. document
kwic_corpus <- corpus(minimum_wage, text = "pre_post")
View(kwic_corpus)
View(kwic_corpus)
kwic_read <- textstat_readability(kwic_corpus, measure = "Flesch.Kincaid")
kwic_read
# More than one measure
kwic_read <- textstat_readability(kwic_corpus, measure = c("Flesch.Kincaid", "Dale.Chall"))
kwic_read
# Add the document identifier and text columns back in
kwic_read <- cbind(minimum_wage$docname, minimum_wage$pre_post, kwic_read)
kwic_read
# Clean up (Delete column)
kwic_read$document <- NULL
# Clean up (Delete column)
kwic_read$document <- NULL
View(kwic_read)
# Create a dfm from the kwic_corpus object
kwic_dfm <- dfm(kwic_corpus)
# Calculate lexical diversity: number of unique tokens per document/number of tokens in document
kwic_lexdiv <- textstat_lexdiv(kwic_dfm)
kwic_lexdiv
# Add the document identifier and text columns back in
kwic_lexdiv <- cbind(minimum_wage$docname, minimum_wage$pre_post, kwic_lexdiv)
# Clean up
kwic_lexdiv$document <- NULL
View(kwic_lexdiv)
# Input is a data frame with text column identified
kwic_sentimentr <- sentiment_by(minimum_wage$pre_post)
View(kwic_sentimentr)
# Use quanteda's built-in sentiment analysis dictionary: http://www.lexicoder.com/
data_dictionary_LSD2015
# Using dfm_lookup function to compare to a dictionary of sentiment.
kwic_sentiment <- dfm_lookup(dfm(kwic_toks_compound), data_dictionary_LSD2015)
View(kwic_sentiment)
# Using dfm_lookup function to compare to a dictionary of sentiment.
kwic_sentiment <- dfm_lookup(dfm(kwic_toks_compound), data_dictionary_LSD2015)
# Compound neg_negative and neg_positive tokens before creating a dfm object
# Function knows something is a bigram.
kwic_toks_compound <- tokens_compound(kwic_toks, data_dictionary_LSD2015)
# Using dfm_lookup function to compare to a dictionary of sentiment.
kwic_sentiment <- dfm_lookup(dfm(kwic_toks_compound), data_dictionary_LSD2015)
View(kwic_sentiment)
kwic_toks_compound
as.data.frame(kwic_toks_compound)
# Compound neg_negative and neg_positive tokens before creating a dfm object
# Function knows something is a bigram.
kwic_toks_compound <- tokens_compound(kwic_toks, data_dictionary_LSD2015)
kwic_toks_compound
help(tokens_compound)
# Using dfm_lookup function to compare to a dictionary of sentiment.
kwic_sentiment <- dfm_lookup(dfm(kwic_toks_compound), data_dictionary_LSD2015)
View(kwic_sentiment)
# Convert kwic_sentiment to a dataframe
kwic_sentiment <- convert(kwic_sentiment, to = "data.frame")
kwic_sentiment
# Add the document identifier column back in
kwic_sentiment <- cbind(minimum_wage$docname, kwic_sentiment)
View(kwic_sentiment)
library(readtext)
library(quanteda)
library(text2vec)
library(sentimentr)
library(tidytext)
library(textdata)
setwd('C:\\Users\\chris.cirelli\\Desktop\\repositories\\gsu_legal_analytics_course')
################################################################################
# Set Directories
################################################################################
dir_base = 'C:\\Users\\chris.cirelli\\Desktop\\repositories\\gsu_legal_analytics_course'
system.file(dir_base, '\\data')
print(system.file(dir_base, '\\data'))
system.file(dir_base, '/data'))
system.file(dir_base, '\data'))
system.file(dir_base, '\\data'))
test <- system.file(dir_base, '\\data'))
test
file.path(dir_base, '\\data')
setwd(dir_hw)
dir_hw
dir_hw = file.path(dir_base, '\\hw\\hw4')
dir_hw
getwd()
################################################################################
# Load Data
################################################################################
help(read_text)
################################################################################
# Load Data
################################################################################
help(readtext)
readtext(paste0(dir_data, "/nlra.txt"))
readtext(paste0(dir_data, "\\nlra.txt"))
dir_data = file.path(dir_base, '\\data')
readtext(paste0(dir_data, "\\nlra.txt"))
nlra.txt <- readtext(paste0(dir_data, "\\nlra.txt"))
nlra
nlra.txt
View(nlra.txt)
View(nlra.txt$text)
nlra.corpus <- corpus(nlra.txt)
library(readtext)
library(quanteda)
library(text2vec)
library(sentimentr)
library(tidytext)
library(textdata)
dir_base = 'C:\\Users\\chris.cirelli\\Desktop\\repositories\\gsu_legal_analytics_course'
dir_data = file.path(dir_base, '\\data')
dir_hw = file.path(dir_base, '\\hw\\hw4')
setwd(dir_hw)
################################################################################
# Load Data
################################################################################
nlra.txt <- readtext(paste0(dir_data, "\\nlra.txt"))
nlra.corpus <- corpus(nlra.txt)
View(nlra.corpus)
nlra.toks <- tokens(nlra.coprus)
################################################################################
# 1.) Get Tokens of Interest
################################################################################
nlra.corpus <- corpus(nlra.txt)
nlra.toks <- tokens(nlra.coprus)
################################################################################
# 1.) Get Tokens of Interest
################################################################################
nlra.corpus <- corpus(nlra.txt$text)
